
\chapter{Summary and Discussion}
\chaptermark{Discussion}
\label{ch:discussion}

The nature of the processing undertaken by the cortical layers needs to be resolved to better understand the computations being performed by our brains \cite{Miller2001}. Functional MRI is the most realistic method to date for studying the cortical layers in living human subjects, as it is spatially precise and non-invasive. The objective of this thesis was to pave the way for doing more robust and routine laminar fMRI analysis. Indeed we made significant steps towards this end. We have developed several new methods that solve major problems in laminar analysis, we implemented a full layer specific analysis, and took explicit care to make the entire analysis pipeline reproducible and reusable.

First, we addressed the problem of local distortions that are often present in Echo Planar Images (EPI). Due to inhomogeneities in the main magnetic field, quantum spins in some areas precess at different rates. Effectively, this causes small shifts in parts of the image with respect to the true position. Thus, even if the true locations of the layers were known in anatomical space, they would be displaced by distortions that can easily be larger than the thickness of the layers. Without correcting this effect it is hopeless to retrieve any reliable layer signal. Chapter~\ref{ch:registration} introduced a method for this type of distortion that can achieve submillimeter accuracy, Recursive Boundary Registration (RBR)

Once the geometry of our cortical surfaces is properly aligned with our functional data, there is a subsequent outstanding question that needs to be answered: how can the laminar signal be extracted from the MRI volume? There are several intuitive ways. A volume could be interpolated at the approximate location of the layers, or voxels can be classified to represent the most likely layer. However, this inherently smears out the laminar signal to some extent. In Chapter~\ref{ch:glm} we set out to quantify this signal leakage and to present a new method to reduce it and to more cleanly separate the laminar signals.

Having prepared the data for high resolution accuracy with RBR, and having invented a way to extract laminar signals with the spatial GLM, the way has been paved to conduct an experimental study. In Chapter~\ref{ch:attention}, we investigated the laminar underpinning of visual attention. A lot is known about the visual system, also in terms of laminar processing, so it is an ideal test scenario for an initial laminar investigation. In general, visual input first goes to the thalamus and thalamo-cortical connections primarily target layer IV in the visual cortex. In contrast, layers I-II and VI are typically implicated in receiving downward information flow (feedback) in a process like attention. We thus investigated whether the BOLD signal indeed shows a laminar differentiation in feedback and feed forward signals.

But could someone else understand, reproduce, or replicate our study from Chapter~\ref{ch:attention}? From the methods section our procedure is clear and could be reproduced, but it would take a lot of time and effort to translate it into code for someone unfamiliar with the analysis. Additionally, the methods describe the core of the analysis pipeline, but not the branches for data quality checks, sanity checks, and intermediate results for, e.g., inspecting registration or layering quality. If the original code is provided, reproduction of the results should be much easier, but still is not straightforward. It takes a lot of time to get accustomed with someone else's style of coding and to the programming languages and toolboxes that were used. And if all these hurdles are overcome, it is still paramount that one has the appropriate software versions and licenses to run the code. We addressed these problems by creating a tool to visually create, inspect, and share analyses, called Porcupine, as shown in Chapter~\ref{ch:porcupine}.

\section*{Chapter~\ref{ch:registration}: Recursive Boundary Registration}
Geometrical transformations from one volume to the other (coregistration) have been very successful for volume-to-volume registration and are used routinely in fMRI analysis. For linear transformations, algorithms may still work well on volumes with low resolution, low contrast, or few slices \cite{Greve2009}, but not for non-linear transformations. The non-linearities require a high number of degrees of freedom because of the many parameters that need to be estimated. This leaves more room for error and therefore requires high contrast data sets (i.e. more information). non-linear transformations are routinely used for transforming single subject anatomical space to a template space with the same strong grey-white matter contrast. However, they are not powerful enough to be applied for cross-contrast purposes to, e.g., undistort low contrast EPI images. We therefore invented a new technique for this: Recursive Boundary Registration (RBR). By recursively applying linear transformations at diminishing spatial scales, we effectively compute a non-linear registration. In order to guarantee smoothness over all transformations, it is combined with a control point lattice that regulates the transformations. Explicitly taking the geometry of the volume into account as a type of prior knowledge is a novel way to approach non-linear registrations.

We tested RBR on two different types of data. First, in order to establish a gold standard with known distortions, we warped a FLASH image that was initially distortion free. Because of the controlled warping, we could easily compare the performance of RBR to our ground truth and verify the quality of the registration. We thus proceeded to examine EPI data set of 11 subjects that had real distortions. As the true size of the distortions was unknown there cannot be an absolute quality metric for the registration. Instead, by adding different levels of noise we showed that there is a clear SNR dependence in the displacement that decreases towards the no-noise condition. Additionally, we provide an abundance of graphical evidence to illustrate the performance of RBR.

The power of the method lies in the fact that it makes explicit use of the cortical folding and the specific geometry of the individual brain. It is therefore more robust to lower contrast and can still produce an accurate registration while preserving the topology of the original surfaces. Because of the large number of parameters that need to be estimated we built in a variety of robustness assurances. Despite the overall improved registration, it remains important to carefully inspect the quality of the registration to verify the required submillimeter accuracy. This proved to be a valuable tool for preparing Gradient Echo images for subsequent analyses and we use it in our experimental study in Chapter~\ref{ch:attention}. It is now an integral part of the fMRI analysis toolbox for laminar fMRI, \url{https://github.com/TimVanMourik/OpenFmriAnalysis}.

A helpful perspective in understanding this method is a more conceptual view on the problem. The problem of coregistration can be classified along two main axes: the type of image contrast can be different or the same, and the required transformation can be linear or non-linear. The easiest scenario is to find a linear transformation for a volume with similar contrast. The problem becomes harder when the volume has a different contrast, as the mapping of intensity values from one volume to the next is unknown. If instead the contrast is the same but the required transformation is non-linear, accurate solutions can still be found \cite{Collins1995}. However, when the contrast is different \emph{and} the registration should be non-linear, the degrees of freedom of the problem increases dramatically, such that the search space becomes too large to estimate. Combine this with the low contrast-to-noise ratio of fMRI data and it is clear that this is computationally infeasible with standard volume-to-volume registration techniques. The trick to be able to do cross-contrast and non-linear is to introduce more prior knowledge into the equation. We here use the geometrical information of the cortex and its many gyri and sulci to more accurately estimate a non-linear cross-modal registration. This assumption only holds for volumes with the same geometry (within subject), so by introducing subject specific prior knowledge, we restrict the number of potential applications. Where the previous non-linear transforms could also compute subject-to-template registration, we lost this ability by strictly enforcing equal geometry across volumes. As a general notion, algorithms can become more powerful when they are more specialised. Usually this requires a specific type of prior knowledge in the data that is quantified and optimised.

\section*{Chapter~\ref{ch:glm}: Spatial GLM for laminar FMRI}
We started out with the notion that all voxels are a mixture of a variety of layers. If this mixture could be accurately modelled it could theoretically also be inverted.
Inverting a model with a General Linear Model (GLM) with voxel specific layer mixtures could yield the layer intensity values. To achieve this, we first set up a mathematical framework to accurately model the layer distribution and subsequently estimate the layer signal intensity. On the modelling perspective, we suggest to incorporate information about the precise location, curvature and thickness of the cortex. For the estimation procedure we suggested taking into account the noise correlation of the volume. Interestingly, we found a way to describe existing procedures in the same mathematical framework of the laminar spatial GLM. This allowed for an easy comparison between procedures in the subsequent validation.

In validating the performance of all methods, we used several different types of data. In order to see if in principle the method would perform better when all assumptions are met, we spent considerable effort to make an accurate simulation of the cortex. By modeling known properties about the curvature into the gyri and sulci of the cortex, we constructed a gold standard of an MRI volume. This allowed for a very clean comparison of the different methods in ideal circumstances. As predicted, extracting the laminar signal by means of a spatial GLM considerably reduced the signal leakage to neighbouring layers. We then investigated the performance of all three methods on two types of functional data: a high resolution (post mortem) piece of cortex to get a large number of cortical layers, and a set of in vivo human structural scans to gauge the viability of the method in live human MRI data. We found largely similar behaviours between methods, but also a disturbing artifact in the spatial GLM profiles: an oscillating pattern that gets stronger mainly when the cortex is divided into more layers. But the performance and the strength of the ringing artifact also depends on a vast parameter set that would be interesting to investigate, amongst which the volume's spatial resolution, noise level, noise type, the presence or absence of veins and intensity gradients, et cetera. Investigating the combination of all these factors would create a combinatoric explosion, so we carefully chose a subset and presented the comparative results.

Thus, we provide a more formal mathematical description of layer separation. Despite the mathematical rigour, the solution is more prone error. We previously mentioned that methods can become more powerful when they use types of prior knowledge inherent to the data. As an important addition, this study shows that it might come at the cost of a high susceptibility to cases where the assumptions are violated. The fact that artifacts appear in the profile for the spatial GLM may well be interpreted as inaccurate boundary placement or the presence of unknown types of noise. While it is the GLM method that amplifies these inaccuracies, it should be noted that they are just as present when other techniques layer extraction methods are used. This may be a serious concern for laminar analysis in general and still requires thorough examination. We thus stress again the importance of carefully inspecting one's data before proceeding to making laminar inferences. Empirically, we find that segmentations with three layers yield stable and plausible solutions for resolutions of approximately [1 mm]$^3$ .

\section*{Chapter~\ref{ch:attention}: Layer Specificity in Visual Attention}
We set up an experiment in which we balanced visual input and attentional input in an orientation discrimination task. We conducted this experiment at high field, 7 Tesla, for higher sensitivity and submillimeter resolution and took explicit care to remove as many sources of noise as possible. We corrected the distortions in the volume with RBR; filtered out heartbeat and respiratory noise with RETROICOR; established regions of interest from a dedicated retinotopic session; had a high number of subjects, a powerful statistical design and performed more rigorous statistical tests than ever conducted in a laminar study; looked at three regions of interest of the visual hierarchy (V1, V2, V3), and performed numerous control analyses. Despite all this, we find no evidence for layer specific differentiation in the BOLD signal.

While we do not find layer statistical differences at the layer level, at the level of the region of interest we do find activation for feed forward and feedback stimuli. In line with previous findings, feedback (attentional) activation increases towards higher visual areas, whereas feed forward (visual) activation decreases \cite{Murray2008,Jehee2011}. Additionally, there is a steady increase towards the superficial layers. This has frequently been observed and has two main potential reasons \cite{Koopmans2010,Polimeni2010}. The blood flows from deep layers to superficial layers and may cause a BOLD carry-over effect. Alternatively, the pial surface has large draining veins to which gradient echo is sensitive that may leak signal into lower layers as a methodological artifact. The lack of layer specific differentiation is hard to be interpreted. When an experiment yields a null results, there are a variety of potential causes that cannot easily be distinguished. They can be separated into three categories: methodological, neurophysiological, and statistical.

As mentioned throughout this thesis, exceedingly precise measurements are required for structures as small as the cortical layers. Along the way of data analysis, many methodological challenges were resolved, but we cannot exclude there to be residual structural misalignments for example due to cortical reconstruction or registration inaccuracies, dynamic misalignments due to movement, or cardiac pulsation of the brain. Additionally, because of the limits of the acquisition we made several compromises. The price we paid for a high spatial resolution and sensitivity was a low SNR per voxel, and reduced specificity as a result of draining veins on top of the cortex. Other choices may have revealed more signal or reduced the amount of noise. Methodological issues that are not sufficiently addressed may in the worst case lead to a bias in the results, and in the best case increase the noise surrounding the effect. If there is a true effect, it will only count as statistically significant if the average effect is stronger than the sources of noise that are inherently present in any measurement. We reduced the sources of noise and tried to increase the signal by scanning a larger number of participants than customary for laminar studies, N=17 (cf. N=6 in \cite{Polimeni2010}, N=4 in \cite{Muckli2015}, N=10 in \cite{Kok2016}). Nonetheless, the signal term may still be small. Neurophysiologically, it is uncertain if targeted layers have a large effect on the laminar BOLD signal. While feed forward and feedback signals preferentially target specific layers, the signal may nearly instantaneously spread to other layers. On top of that, the neurovascular coupling does not guarantee that this is uniquely visible in the respective layer. An alternative explanation could be that our attentional modulation is fundamentally different from other cognitive capacities that have shown positive laminar differentiation, such as figure-ground segregation \cite{Kok2016} or other non-classical receptive field effects \cite{Muckli2015}.

In general, it is hard to specificy the reason behind a null result, but it is interesting to put it in a broader perspective and compare it to other positive results in the literature. Given the difficulty of the analysis, the small effects, and the large number of sources of noise, one might expect many publications with laminar null results. However, nothing could be further from the truth. To date, no paper exists that specifies to have hypothesised and looked for laminar differentiation but found none. 
%rather bold statement, but please prove me otherwise.
It is not hard to understand why this is. It is difficult to interpret negative results because after all, absence of evidence is not evidence of absence. Anything that goes wrong could result in a negative result, whereas it is often perceived that everything was done right when a positive result is in line with a hypothesis. It is thus harder to know what to conclude from a negative result than from a positive result, such that the latter will find an easier path to publication. But as understandable as this may be, it will undoubtedly keep some papers of similar quality in the file drawer, primarily because of their outcomes. Therefore, the state of the literature does not necessarily provide an accurate reflection of all scientific work \cite{Ioannidis2005,Button2013}. A critical reflection on this publication bias would be a welcome addition to the literature.

\section*{Chapter~\ref{ch:porcupine}: Pipelines for FMRI with Porcupine}
By visually creating a pipeline, the conceptual understanding is prioritised over the implementational details. This allows researchers to think more creatively about solving problems and gives them a better overview of their methods. At the same time it solves many problems at the core of the reproducibility crisis in neuroimaging \cite{Nature2017,Munafo2017}. As Porcupine creates analysis code, one can now easily complement the methods section with a visual representation of the pipeline, that automatically translates to a workable script. The graphical pipeline is easily modifiable by non coders, but the produced script could also be used as a template to continue scripting for expert coders. Additionally, we create a minimal operating system as a Docker file. This ensures that identical software versions are used and thus creates a completely reproducible environment.

In fMRI analysis, it is customary that we perform many sequential steps on our data to make it maximally sensitive to the relevant dimensions in the data, and to reduce it to a summary statistic of neurocognitive interest. This type of data analysis is by no means restricted to fMRI analysis alone. It is present in other parts of neuroimaging (e.g. EEG/MEG analysis, \cite{Oostenveld2011}), bioinformatics \cite{Wolstencroft2013}, but also in non-scientific areas such as computer graphics \cite{Blender}. Visual programming tools of Porcupine's kind are also not unique (not even in MRI analysis \cite{Lucas2010}), so it is interesting to consider why they have not been adopted by the community. Part of the reason may be that scientists are interested in, paid to, and judged by doing novel research and inventing new methods for their current scientific problem. As a result, tools that could be of generic use may easily be written for personal use and not necessarily be compatible or consistent with similar software in the field. While common in almost every branch of software engineering, major fMRI analysis toolboxes have dedicated little effort to specifying communication between software components in an application programming interface (API). By and large, this reflects an organisational structure with high degrees of specialisation into independent tasks and a culture where individual accomplishments are valued more than the accomplishments of a team. From the user and developer perspective, the unfortunate consequences are difficulties in getting familiar and maintaining a package when it accumulates knowledge, methods, and new insights. Additionally, it is less than fertile ground for efforts to streamline the analysis, such as visual programming applications. The only neuroimaging package that makes an explicit attempt at consistency throughout its interface is Nipype, so this is the package to which we linked Porcupine. Nipype has the added benefit of linking to major MRI analysis packages in the community, such that it is the most diverse MRI analysis tool to date.

FMRI analyses pipelines increase in complexity and the era of simple blobological studies is over. Logically, the learning curve to get to state of the art methods and cutting edge science is longer than in the early days of fMRI. If we keep reinventing old wheels, this may inadvertently slow down the scientific process of new discoveries. Science will need to find a way to keep these longer pipelines insightful and keep the analysis manageable. With Porcupine we have made a considerable step towards this end. Additionally, it was fully made in the spirit of open science and the software is open source for everybody to contribute. Throughout development, scalability of the software has been a priority. Porcupine can easily be extended to other packages within or outside neuroimaging, but its modular components may also be extended with running pipelines or visualising data. We believe that this is a valuable direction for a more open, reproducible, and sustainable science.

\section{General Discussion}
We here developed a variety of tools for laminar analysis, made it available in an open source toolbox, linked it to our new graphical pipeline interface, and conducted a laminar fMRI experiment. We started out with the goal to make laminar analysis a more routine process and to learn more about directional communication in the cortex. We can safely say that we succeeded in streamlining this process, although we did not find laminar differentiation in our experimental study. Our initial hypothesis was that layer specific BOLD signal would reflect the initial target layers for neuronal input. We have to consider that this picture is too simplified for the complex operations that are being performed by the brain. It is important to further investigate how neuronal input to the cortex propagates to other layers, and how this subsequently reflected in the laminar quantities such as BOLD, but also CBV, CBF, and CMRO$_2$. We have provided the tools to perform laminar analys, independent of contrast mechanism, and now the main outstanding question is: can we use it to investigate the true nature of the laminar signal?

Recent work has shown higher layer specific accuracy for CBV than for BOLD in the motor cortex \cite{Huber2018}. This BOLD/CBV study is a good example of the relevance of comparative studies and could be helpful in getting a better understanding of the neurovascular coupling and neurophysiological processes. It is also interesting to see that the authors report results with 20 intermediate layers and perform control analyses with the spatial GLM. They comment that the GLM method comes with significant noise enhancement, but it may be worth considering that signal to distinguish 20 layers was never there in the first place. With limitation of the data is not revealed in classical interpolation, but oversampling may easily give a false sense of accuracy with respect to the spatial specificity.

In this thesis, we encountered numerous sources of noise in the data as well as in the methods that may obscure true effects or even amplify noise to show up as false effects. After the steps that we took to reduce noise, a logical conclusion could be to increase the signal, for example by investing in even higher field strengths in the future. Unfortunately, this may be subject to the law of diminishing returns. The increase in SNR is attenuated by hardware constraints such that a doubling in field strength only increase by approximately 20\% \cite{?}. 
% @David, specify lower hardware performances? Citation for 20 percent?
And with a linear decrease of the voxel length, its size (and SNR) decreases cubically, so the resolution may not improve much towards higher field strength. Additionally, higher fields typically show higher field inhomogeneities and consequently higher distortions that cannot be corrected with RBR anymore. It will also put higher pressure on hardware and pulse sequences, as the T$_2^*$ -values for tissue, blood and deoxyhemoglobin decrease as a function of field strength. Thus, many technical challenges still need to be overcome before higher fields can adequately aid in the laminar investigation.

Could new developments in the analysis technique bring resolve? The placement of layers is still a point of major improvement. Where we proposed image based distortions correction with RBR, other methods try to use acquisition time strategies with topup \cite{Smith2004}, or acquire structural images with the same distortions \cite{Kashyap2017}. All methods have their upsides and downsides and still stand to be improved. Also the subsequent step of dealing with the partial volume effect with the spatial GLM leaves significant room for improvement. We laid out an initial strategy of creating a layer model and unmixing layer specific signal, but the results still show methodological artifacts and noise amplification. We identified a number of underlying sources, but the analysis stands to benefit from a more thorough investigation and quantification of the factors that contribute to this problem. 

Apart from a better methods and a better understanding of the neurovascular coupling, it is equally important to better understand cognitive processes at the laminar level via other routes. The ultimate goal of laminar analysis is to provide answers for these questions, but as we are still in the early days of laminar fMRI, it goes both ways: in order to validate laminar analysis as a method it is important to have benchmark scenarios against which it can be tested. And at some point, science does not need more big data but big theories \cite{Jensen2014}. More focus on the conceptual and computational nature of the cortical layers is needed to generate testable hypotheses. Only the combination of viable theories and experimental data will give us a better understanding of the human brain and mind. 



%  [Insert somewhat more positive concluding sentence here]
%  ...and world peace
