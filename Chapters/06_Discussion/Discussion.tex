
\chapter{Summary and Discussion}
\chaptermark{Discussion}
\label{ch:discussion}

The nature of the processing undertaken by the cortical layers needs to be resolved to better understand the computations being performed by our brains \cite{Miller2001}. Functional MRI is the most realistic method to date for studying the cortical layers in living human subjects, as it is precise and non-invasive. The objective of this thesis was to pave the way for doing more robust and routine laminar fMRI analysis. Indeed we made significant steps towards this end. We have developed several new methods that solve major problems in laminar analysis, we conducted a full layer specific analysis, and took explicit care to make everything along the way reproducible and reusable.

First, we addressed the problem of local distortions that are often present in Echo Planar Images (EPI). Due to inhomogeneities in the main magnetic field, spins in some areas precess a bit faster or slower than others. Effectively, this causes small shifts in parts of the image with respect to the true position. Thus, even though the true locations of the layers are known, the distortions may easily be larger than the thickness of the layers. Without correcting this effect it is hopeless to retrieve any reliable layer signal. Chapter 2 introduces a method for this type of distortion that can achieve submillimeter accuracy.

Once the geometry of our cortical surfaces is properly aligned with our functional data, there is a subsequent problem: how can the laminar signal be extracted from the MRI volume? There are several intuitive ways. A volume could be interpolated at the approximate location of the layers, or voxels can be classified to represent the most likely layer. However, it is clear that this inherently smears out the laminar signal to some extent. In Chapter 3 we set out to quantify this signal leakage and to present a new method to reduce it and to more cleanly separate the laminar signal.

With the data prepared for high resolution accuracy with Recursive Boundary Registration, and with a way to extract laminar signals with the spatial GLM, the way has been paved to conduct an experimental study. In Chapter 4, we investigate the laminar underpinning of visual attention. A lot is known about the visual system, also in terms of laminar processing, so it is an ideal test scenario for an initial laminar investigation. In general, visual input first goes to the thalamus and thalamo-cortical connections primarily target layer IV in the visual cortex. In contrast, layers I-II and VI are typically implicated in receiving downward information flow (feedback) in a process like attention. We thus investigated if the BOLD signal indeed shows a laminar differentiation in feedback and feed forward signals. 

But how easily could someone else understand, reproduce, or replicate our study from Chapter 4? From the methods section our procedure is clear and could be reproduced, but it would take a lot of time and effort to translate it into code for someone unfamiliar with the analysis. Additionally, the methods describe the core of the analysis pipeline, but not the branches for data quality checks, sanity checks, and intermediate results for e.g. inspecting registration or layering quality. If the original code is provided, reproduction of the results should be much easier, but still is not straightforward. It takes a lot of time to get accustomed with someone else's style of coding and to the programming languages and toolboxes that were used. And if all these hurdles are overcome, it is still paramount that one has the appropriate software versions and licenses to run the code. We address these problems by creating a tool to visually create, inspect, and share analyses, called Porcupine, as shown in Chapter 5.

\section*{Chapter 2: Recursive Boundary Registration}
Geometry transformations from one volume to the other (coregistration) have been very successful for volume-to-volume registration and are used routinely in fMRI analysis. For linear transformations algorithms may still work well on volmes with low resolution, low contrast, or few slices \cite{Greve201}, but not for non linear transformations. The non linearities require a high number of degrees of freedom because of the many parameters that need to be estimated. This leaves much more room for error and therefore requires high contrast data sets. Non linear transformations are routinely used for transforming single subject anatomical space to a template space with the same strong grey-white matter contrast. However, they are not powerful enough to be applied for cross-contrast purposes to e.g. undistort low contrast EPI images. We therefore invented a new technique for this, Recursive Boundary Registration (RBR). By recursively applying linear transformations on diminishing spatial scales, we effectively compute a non linear registration. In order to guarantee smoothness over all transformations, it is combined with a control point lattice that regulates the transformations. Explicitly taking the geometry of the volume into account as a type of prior knowledge is a novel way to approach non linear registrations.

We tested RBR on two different types of data. First, in order to establish a gold standard with known distortions, we warped a FLASH image that was initially distortion free. Because of the controlled warping, we could easily compare the performance of RBR to our ground truth and verify the quality of the registration. We thus proceeded to EPI data set of 11 subjects that had real distortions. As the true size of the distortions was unknown there cannot be an absolute quality metric for the registration. Instead, by adding different levels of noise we showed that there is a clear SNR dependence in the displacement that decreases towards the no-noise condition. Additionally, we provide an abundance of graphical evidence to illustrate the performance of RBR.

The power of the method lies in the fact that it makes explicit use of the cortical folding and the specific geometry of the individual brain. It is therefore more robust to lower contrast and can still produce an accurate registration while preserving the topology of the original surfaces. Because of the large number of parameters that needs to be estimated we built in a variety of robustness assurances. Despite the overall improved registration, it is still important to carefully inspect the quality of the registration to verify the required submillimetre accuracy. This proved to be a valuable tool for preparing Gradient Echo images for subsequent analyses and we use it in our experimental study in Chapter 4. It is now an integral part of the fMRI analysis toolbox for laminar fMRI, \url{https://github.com/TimVanMourik/OpenFmriAnalysis}.

A helpful perspective in understanding this method is a more conceptual look on the problem. The problem of coregistration can be classified along two main axes: the type of image contrast can be different or the same, and the required transformation can be linear or non linear. The easiest scenario is to find a linear transformation for a volume with similar contrast. The problem becomes harder when the volume has a different contrast, as the mapping of intensity values from one volume to the next is unknown. If instead the contrast is the same but the required transformation is non linear, it is still solvable to a good approximation \cite{Collins1995}. However, when the contrast is different \emph{and} the registration should be non linear, the degrees of freedom of the problem increases drastically, such that the search space becomes too large to estimate. Combine this with the low contrast-to-noise ratio of fMRI data and it is clear that this is a hopeless endeavour with standard volume-to-volume registration techniques. The trick to be able to do cross-contrast and non linear is to introduce more prior knowledge into the equation. We here use the geometrical information of the cortex and its many gyri and sulci to more accurately estimate a non linear cross-modal registration. This assumption only holds for volumes with the same geometry (within subject), so by introducing subject specific prior knowledge, we restrict the number of potential applications. Where the previous non-linear transforms could also compute subject-to-template registration, we lost this ability by strictly enforcing equal geometry across volumes. As a general notion, algorithms can become more powerful when they are more specialised. Usually this requires a specific type of prior knowledge in the data that is quantified and optimised.

\section*{Chapter 3: Spatial GLM for laminar FMRI}
We started out with the notion that all voxels are a mixture of a variety of layers. If this mixture could be accurately modelled it could theoretically also be inverted.
Inverting a model with a General Linear Model (GLM) with voxel specific layer mixtures could yield the layer intensity values. To achieve this, we first set up a mathematical framework to accurately model the layer distribution and subsequently estimate the layer signal intensity. On the modelling perspective, we suggest to incorporate information about the precise location, curvature and thickness of the cortex. For the estimation procedure we suggested taking into account the noise correlation of the volume. Interestingly, we found a way to describe existing procedures in the same mathematical framework of the laminar spatial GLM. This allowed for an easy comparison between procedures in the subsequent validation.

In validating the performance of all methods, we used several different types of data. In order to see if in principle the method would perform better when all assumptions are met, we spent considerable effort to make an accurate simulation of the cortex. By modeling known properties about the curvature into the gyri and sulci of the cortex, we constructed a gold standard of an MRI volume. This allowed for a very clean comparison of the different methods in ideal circumstances. As predicted, extracting the laminar signal by means of a spatial GLM considerably reduced the signal leakage to neighbouring layers. We then investigated the performance of all methods on two types of functional data: a high resolution (post mortem) piece of cortex to get a large number of cortical layers, and a set of in vivo human structural scans to gauge the viability of the method in live human MRI data. We found largely similar behaviours between methods, but also a disturbing kind of artifact in the spatial GLM profiles: an oscillating pattern that gets stronger mainly when the cortex is divided into more layers. But the performance and the strength of the ringing artifact also depends on a vast parameter set that would be interesting to investigate, amongst which the volume's spatial resolution, noise level, noise type, the presence or absence of veins and intensity gradients, et cetera. Investigating the combination of all these factors would create a combinatoric explosion, so we carefully chose a subset and presented the comparative results.

Thus, we provide a more formal mathematical description of layer separation. Despite the mathematical rigour, the solution is more prone error. We previously mentioned that methods can become more powerful when they use types of prior knowledge inherent to the data. As an important addition, this study shows that it might come at the cost of a high susceptibility to cases where the assumptions are violated. The fact that artifacts appear in the profile for the spatial GLM may well be interpreted as inaccurate boundary placement or the presence of unknown types of noise. While it is the GLM method that amplifies these inaccuracies, it should be noted that they are just as present when other techniques layer extraction methods are used. This may be a serious concern for laminar analysis in general and still requires thorough examination. We thus stress again the importance of carefully inspecting one's data before proceeding to making laminar inferences. 

\section*{Chapter 4: Layer Specificity in Visual Attention}
We set up an experiment in which we balanced visual input and attentional input in an orientation discrimination task. We conducted this at high field, 7 Tesla, for higher sensitivity and submillimeter resolution and took explicit care to remove as many sources of noise as possible. We corrected the distortions in the volume with RBR; filtered out heartbeat and respiratory noise with RETROICOR; established regions of interest from a dedicated retinotopic session; had a high number of subjects, a more powerful statistical design and performed more rigorous statistical tests than ever conducted in a laminar study; looked at three regions of interest of the visual hierarchy (V1, V2, V3), and performed numerous control analyses. Despite all this, we find no evidence for layer specific differentiation in the BOLD signal.

While we do not find layer statistical differences at the layer level, at the level of the region of interest we do find activation for feed forward and feedback stimuli. In line with previous findings, feedback (attentional) activation increases towards higher visual areas, whereas feed forward (visual) activation decreases \cite{Murray2008,Jehee2011}. Additionally, there is a steady increase towards the superficial layers. This has frequently been observed and has two main potential reasons \cite{Koopmans2010,Polimeni2010}. The blood flows from deep layers to superficial layers and may cause a BOLD carry-over effect. Alternatively, the pial surface has large draining veins to which gradient echo is sensitive that may leak signal into lower layers as a methodological artifact. The lack of layer specific differentiation is hard to be interpreted. When an experiment yields a null results, there is a variety of potential causes that cannot easily be distinguished. They can be separated into three categories: methodological, neurophysiological, and statistical. 

As mentioned throughout this thesis, exceedingly precise measurements are required for structures as small as the cortical layers. Along the way of data analysis, many methodological challenges were resolved, but we cannot exclude there to be e.g. residual structural misalignments due to cortical reconstruction or registration inaccuracies, dynamic misalignments due to movement or cardiac pulsation of the brain. Additionally, because of the limits of the acquisition we made several compromises. The price we paid for a high spatial resolution and sensitivity was a low SNR per voxel, and reduced specificity as a result of draining veins on top of the cortex. Other choices may have revealed more signal or reduced the amount of noise. Methodological issues that are not sufficiently addressed may in the worst case lead to a bias in the results, and in the best case increase the noise surrounding the effect. If there is a true effect, it will only count as statistically significant if the average effect is stronger than the sources of noise that are inherently present in any measurement. We reduced the sources of noise to and tried to increase the signal by scanning a larger number of participants than customary for laminar studies, N=17 (cf. N=6 in \cite{Polimeni2010}, N=4 in \cite{Muckli2015}, N=10 in \cite{Kok2016}). Nonetheless, the signal term may still be small. Neurophysiologically, it is uncertain if targeted layers have a large effect on the laminar BOLD signal. While feed forward and feedback signals preferentially target specific layers, the signal may nearly instantaneously spread to other layers. On top of that, the neurovascular coupling does not guarantee that this is uniquely visible in the respective layer. An alternative explanation could be that our attentional modulation is fundamentally different from other cognitive capacities that have shown positive laminar differentiation, such as figure-ground segregation \cite{Kok2015} or other non-classical receptive field effects \cite{Muckli2015}. 

In general, it is hard to specificy the reason behind a null result, but it is interesting to put it in a broader perspective and compare it to other positive results in the literature. Given the difficulty of the analysis, the small effects, and the large number of sources of noise, one might expect many publications with laminar null results. However, nothing could be further from the truth. To date, no paper exists that specifies to have hypothesised and looked for laminar differentiation but found none. %rather bold statement, but please prove me otherwise.
It is not hard to understand why this is. It is difficult to interpret negative results because after all, absence of evidence is not evidence of absence. The space of possibilities by which a study can yield negative results is a lot larger than by which it can yield positive results. It is thus a lot harder to know what to conclude from a negative result than from a positive result, such that the latter will find an easier path to publication. But as understandable as this may be, it will undoubtedly keep some papers of similar quality in the file drawer, primarily because of their outcomes. Therefore, the state of the literature does not necessarily provide an accurate reflection of all scientific work. A critical reflection on this publication bias would be a welcome addition to the literature. 

\section*{Chapter 5: Pipelines for FMRI with Porcupine}
By visually creating a pipeline, the conceptual understanding is prioritised over the implementational details. This allows researchers to think more creatively about solving problems and gives them a better overview of their methods. At the same time it solves many problems at the core of the reproducibility crisis in neuroimaging \cite{Nature2017,...}. As Porcupine creates analysis code, one can now easily complement the methods section with a visual representation of the pipeline, that automatically translates to a workable script. The graphical pipeline is easily modifiable by non coders, but the produced script could also be used as a template to continue scripting for expert coders. Additionally, we create a minimal operating system as a Docker file. This ensures that identical software versions are used and thus creates a completely reproducible environment.

In fMRI analysis, it is customary that we perform many subsequent steps on our data to make it maximally sensitive to the relevant dimensions in the data, and to reduce it to a summary statistic of neurocognitive interest. This type of data analysis is by no means restricted to fMRI analysis alone. It is present in other parts of neuroimaging (e.g. EEG/MEG analysis, \cite{Oostenveld2011}), bioinformatics \cite{Wolstencroft2013}, but also in non-scientific areas such as computer graphics \cite{Blender}. Visual programming tools of Porcupine's kind are also not unique (not even in MRI analysis \cite{Lucas2010}), so it interesting to consider why they have not been adopted by the community. Part of the reason may be that scientists are interested in, paid to, and judged by doing novel research and invent new methods for their current scientific problem. As a result, tools that could be of generic use may easily be written for personal use and not necessarily be compatible or consistent with similar software in the field. While common in almost every branch of software engineering, major fMRI analysis toolboxes have dedicated little effort to specifying communication between software components in an application programming interface (API). By and large, this reflects an organisational structure with high degrees of specialisation into independent tasks and a culture where individual accomplishments are valued more than the accomplishments of a team. From the user and developer perspective, the unfortunate consequences are difficulties in getting familiar and maintaining a package when it accumulates knowledge, methods, and new insights. Additionally, it is less than fertile ground for efforts to streamline the analysis, such as visual programming applications. The only neuroimaging package that makes an explicit attempt at consistency throughout its interface is Nipype, so this is the package to which we linked Porcupine. Nipype has the added benefit of linking to major MRI analysis packages in the community, such that it is the most diverse MRI analysis tool to date. 

FMRI analyses pipelines increase in complexity and the era of simple blobological studies is over. Logically, the learning curve to get to state of the art methods and cutting edge science is longer than in the early days of fMRI. If we keep reinventing old wheels, this may inadvertently slow down the scientific process of new discoveries. Science will need to find a way to keep these longer pipelines insightful and keep the analysis manageable. With Porcupine we have made a considerable step towards this end. Additionally, it was fully made in the spirit of open science and the software is open source for everybody to contribute. Throughout development, scalability of the software has been a priority. Porcupine can easily be extended to other packages within or outside neuroimaging, but its modular components may also be extended with running pipelines or visualising data. We believe that this is a valuable direction for a more open, reproducible, and sustainable science.







\section{Conclusion}
We here developed a variety of tools for laminar analysis, made it available in an open source toolbox, linked it to our new graphical pipeline interface, and conducted a laminar fMRI experiment. 

We started out with the goal to make laminar analysis a more routine process to explore new depths of the cortex and learn more about directional communication in the cortex.

 We can safely say that we succeeded in streamlining this process, although we did not find laminar differentiation in out experimental study.

 We hope that the pipeline that we created will be used to continue investigating the cortical layers, but that users will remain critical about their findings. In this thesis, we encountered numerous sources of noise in the data as well as in the methods that may obscure true effects or even amplify noise to show up as false effects. Gains in SNR and spatial resolution would still be extremely helpful to make laminar analysis more robust. 


There is still much to improve in terms of 

distortion correction is useful, but it would be better if there weren't anything to correct. 

but also cognitive theories that go deeper into 
and a more precise interpretation of the laminar signal


But many challenges in spatial and temporal resolution, interpretation, and all sorts of noise have to be overcome before this can be easily used to solve neuropsychological problems. 

What is the nature of the laminar signal?

\linespread{1.5}
\newpage






